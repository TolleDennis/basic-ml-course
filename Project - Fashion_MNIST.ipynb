{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TolleDennis/basic-ml-course/blob/master/Project%20-%20Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R19yERDu-AU7"
      },
      "source": [
        "## **STATEMENT PROBLEM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nILFm-GckmL"
      },
      "source": [
        "## **IMAGE CLASSIFICATION: FashionMNIST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK-7LSlOZxD7"
      },
      "source": [
        "## In order to categorize outfits and apparel from simply photos during the production process, visual analysis challenges are typically encountered in the fashion business. The visual inspection of the various results of a designer's work, which are stored in a collecting bucket before moving on to categorization, is the most crucial step to do in order to establish efficient marketing plans."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IEqY-UqUmxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E549Z2sX3GCY"
      },
      "source": [
        "## Our immediate strategy in this project is to investigate how Deep Learning techniques like Convolutional Neural Networks (CNN) and Multilayer Perception (MLP) can be used to classify Fashion MNIST photos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmsjZ_j16TpR"
      },
      "source": [
        "## **PURPOSE OF THE PROJECT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snPzvsxP9jnb"
      },
      "source": [
        "## The major objectives of this project are to educate ourselves on the complexities of CNN and MLP, to grasp and explain them, and to gain recognition for our efforts as a team."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQPjjIK_9t4a"
      },
      "source": [
        "## In order to optimize for more insights, the project includes background information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSJl49xy3FvX"
      },
      "source": [
        "## **EXPLORATION OF DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDL08quo3FcJ"
      },
      "source": [
        "## Importing the libraries needed for this classification project allows us to begin exploring the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlVdUznhIQ9x"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_printoptions(linewidth=120)\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfOxGqE7UH4C"
      },
      "source": [
        "## More classes drawn from several libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tikKH43t0TZn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from torch import nn,optim,no_grad\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHzfBqK_diLU"
      },
      "source": [
        "## **Loading data from torchvision - CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFGDg7ycUm5W"
      },
      "source": [
        "## The DataLoader is then used to preprocess the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSBYk6oZdbMf",
        "outputId": "019721d4-cb59-4938-b077-9a33551e5a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16179683.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 266092.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5004950.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6380696.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_set1 = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=True, transform=\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\n",
        "test_set1 = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()]))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abrP5UplU03c"
      },
      "source": [
        "## How many samples you'll feed in a single batch. As a result, training takes less time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsKhwS40U5OU"
      },
      "source": [
        "## Data from the train and test sets are split at random via a shuffle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMAxihY9cPgI"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set1, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set1, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESnUXownVEjE"
      },
      "source": [
        "##Our data set contains 60000 images and the number of images we are testing is 10000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cRMwrApSqHE",
        "outputId": "1e7a2a56-e5b1-41c8-9fdd-f8576c32fb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length :  60000\n",
            "Testing data length :  157\n"
          ]
        }
      ],
      "source": [
        "print('Training data length : ', len(train_set1))\n",
        "\n",
        "print('Testing data length : ', len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt2Gw91jVB7m"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1VWgCUKVs3B"
      },
      "source": [
        "## **DATA VISUALIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCkt_LClV3Hi"
      },
      "source": [
        "## Python has built-in functions for working with iterables, such as next() and iter(). The __iter__() method on the iterable is called throughout the execution of a for loop, and the resulting iterator is then passed through the __next__() method until the end of the iterator is reached. After that, the loop ends and it raises a stopIteration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the transformation to apply to the images\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_set1 = datasets.MNIST('mnist_data/', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create a data loader for the training set\n",
        "sample_images = torch.utils.data.DataLoader(train_set1, batch_size=64)\n",
        "\n",
        "# Get a batch of images and labels\n",
        "batch = next(iter(sample_images))\n",
        "images, labels = batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_oMoFllWe6r",
        "outputId": "c8715a5e-98eb-4161-ccc4-724d94a04d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 99200234.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 16676169.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26795187.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5230787.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "BD8h1M47VX1y",
        "outputId": "f5c89545-a4df-463c-ea74-c15717a297e2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d8a2b87dffa1>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Get a batch of images and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Plot the first 20 images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the transformation to apply to the images\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_set = datasets.MNIST('mnist_data/', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create a data loader for the training set\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "# Get a batch of images and labels\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Plot the first 20 images\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "for idx in range(20):\n",
        "    ax = fig.add_subplot(4, 20/4, idx+1,xticks = [],yticks = [])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap=\"gray\")\n",
        "    ax.set_title(labels[idx].item())\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sample_images = torch.utils.data.DataLoader(train_set1, batch_size=64)\n",
        "\n",
        "batch = next(iter(sample_images))\n",
        "images, labels = batch\n",
        "\n",
        "fig =plt.figure(figsize=(15,5))\n",
        "for idx in range(20):\n",
        "  ax = fig.add_subplot(4, 20/4, idx+1,xticks = [],yticks = [])\n",
        "  ax.imshow(np.squeeze(images[idx]),cmap = \"gray\")\n",
        "\n",
        "  ax.set_title(labels[idx].item())\n",
        "  fig.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJb5_5PIWB4P"
      },
      "source": [
        "## This dataset contains 10 different styles of clothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TeXQajsIQqz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "output_mapping = {\n",
        "                 0: \"T-shirt/Top\",\n",
        "                 1: \"Trouser\",\n",
        "                 2: \"Pullover\",\n",
        "                 3: \"Dress\",\n",
        "                 4: \"Coat\", \n",
        "                 5: \"Sandal\", \n",
        "                 6: \"Shirt\",\n",
        "                 7: \"Sneaker\",\n",
        "                 8: \"Bag\",\n",
        "                 9: \"Ankle Boot\"\n",
        "                 }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSNpFrL3WKYf"
      },
      "source": [
        "## Mapping 20 images with their respective labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utv4yR2o26de"
      },
      "outputs": [],
      "source": [
        "images,labels=next(iter(train_loader))\n",
        "fig=plt.figure(figsize=(25,4))\n",
        "for i in range(1,21):\n",
        "    ax=fig.add_subplot(2,10,i,xticks=[],yticks=[])\n",
        "    ax.imshow(images[i].view(28,28),cmap='gray')\n",
        "    ax.set_title(str(output_mapping[labels[i].item()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb0iykizWmV3"
      },
      "source": [
        "## **FEAUTURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4OxGJAWmHm"
      },
      "source": [
        "## Feature engineering is the process of employing statistical or machine learning techniques to transform unprocessed information into desired characteristics. A machine learning technique called feature engineering uses data to generate new variables that aren't present in the training set. With the aim of streamlining and accelerating data transformations while also improving model accuracy, it can generate new features for both supervised and unsupervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiPnXg-l3zjj"
      },
      "outputs": [],
      "source": [
        "sample=list(iter(test_loader))\n",
        "images,labels=sample[0]\n",
        "images,labels=sample[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCBw4AMDXM3y"
      },
      "source": [
        "## **MODELING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAxDeVHGXMlw"
      },
      "source": [
        "## The development, training, and application of machine learning algorithms that simulate logical decision-making based on accessible facts are known as modeling. Advanced intelligence approaches including real-time analytics, predictive analytics, and augmented analytics are supported by Ml algorithms, which act as a foundation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6e-YUcVdvRW"
      },
      "outputs": [],
      "source": [
        "def get_item(preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
        "\n",
        "@torch.no_grad() \n",
        "def get_all_preds(network, dataloader):\n",
        "    all_preds = torch.tensor([])\n",
        "    model = network\n",
        "    for batch in dataloader:\n",
        "        images, labels = batch\n",
        "        preds = model(images) \n",
        "        all_preds = torch.cat((all_preds, preds), dim=0)\n",
        "        \n",
        "    return all_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JBWg9rnXdVM"
      },
      "source": [
        "## Defining the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzncO0y6yh0x"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoMU8BuUg5Hh"
      },
      "source": [
        "## **Modelling with MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjo6P2eqYhaz"
      },
      "outputs": [],
      "source": [
        "class FashionMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMLP,self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512,512)\n",
        "        self.fc3 = nn.Linear(512,10)\n",
        "        self.droput = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.droput(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.droput(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim =1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZGcxSB_e5nP"
      },
      "source": [
        "## **Hyperameters - LR and loss function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHOFfPY4X4iV"
      },
      "source": [
        "## In order to go toward a loss function minimum, an optimization algorithm's learning rate controls the step size at each iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRPYr_M0YKrs"
      },
      "source": [
        "## How successfully your machine learning algorithm predicts the featured data set is assessed using the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHbCUdk_YSma"
      },
      "source": [
        "## We are using an optimizer SGD, with a learning rate of 0.001 using the MLP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QucnYgwi_5z0"
      },
      "outputs": [],
      "source": [
        "#learning rate 0.001 and lss is SGD\n",
        "MLPmodel1 = FashionMLP()\n",
        "\n",
        "error1 = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate1 = 0.001\n",
        "optim = torch.optim.SGD(MLPmodel1.parameters(),lr = learning_rate1)\n",
        "print(MLPmodel1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GBydlPXYm0V"
      },
      "source": [
        "## We are using an optimizer Adam, with a learning rate of 0.005 using the MLP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deO0nQp7fZO8"
      },
      "outputs": [],
      "source": [
        "#learning rate 0.005 and lss is Adam\n",
        "MLPmodel2 = FashionMLP()\n",
        "\n",
        "error2 = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.Adam(MLPmodel2.parameters(), lr=learning_rate)\n",
        "print(MLPmodel2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcv4Z5eUY-Ct"
      },
      "source": [
        "## **TRAINING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIlgtZyoZNnG"
      },
      "source": [
        "## We employ an optimizer repeatedly throughout training in order to obtain the correct response. An optimizer is a set of algorithms or techniques used to modify the neural network's weights and learning rate in order to minimize losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBB8IbTmZXnl"
      },
      "source": [
        "## Epoch is the total number of cycles that all of the training data go through in order to train a machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLMTvWtcZudB"
      },
      "source": [
        "## Traing the MLP model using a learning rate 0.001 and an optimizer SGD to get the highest accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mv1SYsqas0g"
      },
      "source": [
        "## Accuracy increased to 78.8%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgrQGtoOiWp1"
      },
      "outputs": [],
      "source": [
        "#Training model with params (lr = 0.001,optimizer = SGD)\n",
        "epochs = 100\n",
        "highest_accuracy = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = MLPmodel1(imgs) \n",
        "        loss = error1(preds, lbls) \n",
        "        optim.zero_grad() \n",
        "        loss.backward()  \n",
        "        optim.step() \n",
        "        \n",
        "    for batch in test_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = MLPmodel1(imgs)\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_item(preds, lbls)\n",
        "        accuracy = (total_correct/len(test_set1)) * 100\n",
        "        \n",
        "    end_time = time.time() - start_time    \n",
        "    print(\"Epoch no.\",epoch+1 ,\"| Validation accuracy: \",round(accuracy,0),\"%\", \"| total_correct: \", total_correct,\"/10000\" ,\"| epoch_dura: \", end_time,\"sec\")\n",
        "    \n",
        "    if accuracy >= highest_accuracy:\n",
        "        print('Validation Accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        highest_accuracy,\n",
        "        accuracy))\n",
        "        torch.save(MLPmodel1.state_dict(), 'MLPmodel1.pt')\n",
        "        highest_accuracy = accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "She9LmZ-aFoo"
      },
      "source": [
        "## Traing the MLP model using a learning rate 0.005 and an optimizer Adam to get the highest accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zkxHA-Uafny"
      },
      "source": [
        "## Accuracy increased to 85%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyIpMeECir3C"
      },
      "outputs": [],
      "source": [
        "#Training model with params (lr = 0.005,optimizer = Adam)\n",
        "epochs = 100\n",
        "highest_accuracy = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = MLPmodel2(imgs) \n",
        "        loss = error1(preds, lbls) \n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer.step() \n",
        "        \n",
        "    for batch in test_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = MLPmodel2(imgs)\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_item(preds, lbls)\n",
        "        accuracy = (total_correct/len(test_set1)) * 100\n",
        "        \n",
        "    end_time = time.time() - start_time    \n",
        "    print(\"Epoch no.\",epoch+1 ,\"| Validation accuracy: \",round(accuracy,0),\"%\", \"| total_correct: \", total_correct,\"/10000\" ,\"| epoch_dura: \", end_time,\"sec\")\n",
        "    \n",
        "    if accuracy >= highest_accuracy:\n",
        "        print('Validation Accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        highest_accuracy,\n",
        "        accuracy))\n",
        "        torch.save(MLPmodel2.state_dict(), 'MLPmodel2.pt')\n",
        "        highest_accuracy = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS0wZtd7lqdf"
      },
      "outputs": [],
      "source": [
        "MLPmodel1.load_state_dict(torch.load('MLPmodel1.pt'))\n",
        "MLPmodel2.load_state_dict(torch.load('MLPmodel2.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzfl2yYvYlW-"
      },
      "source": [
        "## **MLP Model1 (LR = 0.001, Loss = SGD) Confusion Matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW43L9IkbGwz"
      },
      "source": [
        "## Plotting the confusion matrix graph using a learning rate of 0.001 and an optimizer SGD, "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VJcu86MoxudJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyBV-q_9bSeR"
      },
      "source": [
        "## We found an accuracy of 0.8231 and a mismatch of 0.1769"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOF-1JtEt91V"
      },
      "outputs": [],
      "source": [
        "pred_data_loader = torch.utils.data.DataLoader(batch_size=1000, dataset=test_set1, num_workers=2)\n",
        "all_preds= get_all_preds(network=MLPmodel1, dataloader=pred_data_loader) \n",
        "plot_confusion_matrix(cm=confusion_matrix(y_true=test_set1.targets, y_pred=all_preds.argmax(1)), target_names=test_set1.classes, normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfnG6Yhj9R_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpD0Vg2WY_DI"
      },
      "source": [
        "## **MLP Model2 (LR = 0.005, Loss = Adam) Confusion Matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBSCudn-bm73"
      },
      "source": [
        "## Plotting the confusion matrix graph using a learning rate of 0.005 and an optimizer Adam,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UG-FFVUbyke"
      },
      "source": [
        "## We found an accuracy of 0.8651 and a mismatch of 0.1349, More efficient than the first MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLTHeecwYUvu"
      },
      "outputs": [],
      "source": [
        "pred_data_loader = torch.utils.data.DataLoader(batch_size=1000, dataset=test_set1, num_workers=2)\n",
        "all_preds= get_all_preds(network=MLPmodel2, dataloader=pred_data_loader) \n",
        "plot_confusion_matrix(cm=confusion_matrix(y_true=test_set1.targets, y_pred=all_preds.argmax(1)), target_names=test_set1.classes, normalize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5HTMtxYo3Yr"
      },
      "source": [
        "## **Modelling with CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWHWMeCcbVr"
      },
      "source": [
        "## The CNN model invovles 3 layers: Input Layer, HIdden layer and the Output layer. The input layer receives the images, while the hidden layer performs most of the computation and involves other layers such as the CNN layer, pooling layer and the flattening layer, and finally the output layer which gives the image predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FfbkQE7cRMY"
      },
      "source": [
        "## Input layer module tf.reshape reshapes and shapes the tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB0EA2Jpen90"
      },
      "source": [
        "##Modelling with two CNN Models to get the one with best Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDat8EWg3zmn"
      },
      "outputs": [],
      "source": [
        "class FashionModel1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(FashionModel1, self).__init__()\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjLW9v76G5JL"
      },
      "outputs": [],
      "source": [
        "class FashionModel2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FashionModel2,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size=5)\n",
        "    self.fc1 = nn.Linear(in_features=12*4*4,out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120,out_features=60)\n",
        "    self.fc3 = nn.Linear(in_features=60,out_features=40)\n",
        "    self.out = nn.Linear(in_features=40,out_features=10)\n",
        "  def forward(self,x):\n",
        "    #input layer\n",
        "    x = x\n",
        "    #first hidden layer\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "    #second hidden layer\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "    #third hidden layer\n",
        "    x = x.reshape(-1,12*4*4)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    #fourth hidden layer\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    #fifth hidden layer\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    #output layer\n",
        "    x = self.out(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwiX-mmxE_zl"
      },
      "source": [
        "## **Hyperameters - LR and loss function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wd4b8mshCSp"
      },
      "source": [
        "## We using the SAM optimizer as well as SGD and Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8qJgBCh0wpf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_p5bAONhq9d"
      },
      "source": [
        "## Hypeparameters - learning rate of [0.001, 0.005, 0.1] and Optimizer Adam, SGD and SAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-zhXQyP-He1"
      },
      "outputs": [],
      "source": [
        "#learning rate 0.001 and loss function is Adam\n",
        "\n",
        "CNNmodel1_1= FashionModel1()\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate1 = 0.001\n",
        "optimizer = torch.optim.Adam(CNNmodel1_1.parameters(), lr=learning_rate1)\n",
        "\n",
        "print(CNNmodel1_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNeQqRwg6Jgs"
      },
      "outputs": [],
      "source": [
        "#Training model with params (lr = 0.001,optimizer = Adam)\n",
        "epochs = 15\n",
        "highest_accuracyModel1_1 = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel1_1(imgs) \n",
        "        loss = error(preds, lbls) \n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer.step() \n",
        "\n",
        "    for batch in test_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel1_1(imgs)\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_item(preds, lbls)\n",
        "        accuracy = (total_correct/len(test_set1)) * 100\n",
        "        \n",
        "    end_time = time.time() - start_time    \n",
        "    print(\"Epoch no.\",epoch+1 ,\"| accuracy: \", accuracy,\"%\", \"| total_correct: \", total_correct,\"/10000\" ,\"| epoch_dura: \", end_time,\"sec\")\n",
        "    \n",
        "    if accuracy >= highest_accuracyModel1_1:\n",
        "        print('Accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        highest_accuracyModel1_1,\n",
        "        accuracy))\n",
        "        torch.save(CNNmodel1_1.state_dict(), 'CNNmodel1_1.pt')\n",
        "        highest_accuracyModel1_1 = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08kD8Vy-0m3n"
      },
      "outputs": [],
      "source": [
        "#learning rate 0.005 and loss function is SGD\n",
        "CNNmodel1_2 = FashionModel1()\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate2 = 0.005\n",
        "optimizer2 = torch.optim.Adam(CNNmodel1_2.parameters(), lr=learning_rate2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeg15i-Lgb57"
      },
      "outputs": [],
      "source": [
        "#Training model with params (lr = 0.001,optimizer = Adam)\n",
        "epochs = 50\n",
        "highest_accuracyModel1_2 = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel1_2(imgs) \n",
        "        loss = error(preds, lbls) \n",
        "        optimizer2.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer2.step() \n",
        "\n",
        "    for batch in test_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel1_2(imgs)\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_item(preds, lbls)\n",
        "        accuracy = (total_correct/len(test_set1)) * 100\n",
        "        \n",
        "    end_time = time.time() - start_time    \n",
        "    print(\"Epoch no.\",epoch+1 ,\"| accuracy: \", accuracy,\"%\", \"| total_correct: \", total_correct,\"/10000\" ,\"| epoch_dura: \", end_time,\"sec\")\n",
        "    \n",
        "    if accuracy >= highest_accuracyModel1_2:\n",
        "        print('Accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        highest_accuracyModel1_2,\n",
        "        accuracy))\n",
        "        torch.save(CNNmodel1_2.state_dict(), 'CNNmodel1_2.pt')\n",
        "        highest_accuracyModel1_2 = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q13j3dPPzS8"
      },
      "outputs": [],
      "source": [
        "CNNmodel2_1 = FashionModel2()\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate1 = 0.001\n",
        "optimizer3 = torch.optim.Adam(CNNmodel2_1.parameters(), lr=learning_rate1)\n",
        "\n",
        "print(CNNmodel2_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd8UxqUrxk-K"
      },
      "outputs": [],
      "source": [
        "#Training model with params (lr = 0.001,optimizer = Adam)\n",
        "epochs = 50\n",
        "highest_accuracyModel2_1 = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel2_1(imgs) \n",
        "        loss = error(preds, lbls) \n",
        "        optimizer3.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer3.step() \n",
        "\n",
        "    for batch in test_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel2_1(imgs)\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_item(preds, lbls)\n",
        "        accuracy = (total_correct/len(test_set1)) * 100\n",
        "        \n",
        "    end_time = time.time() - start_time    \n",
        "    print(\"Epoch no.\",epoch+1 ,\"| accuracy: \", accuracy,\"%\", \"| total_correct: \", total_correct,\"/10000\" ,\"| epoch_dura: \", end_time,\"sec\")\n",
        "    \n",
        "    if accuracy >= highest_accuracyModel2_1:\n",
        "        print('Accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        highest_accuracyModel2_1,\n",
        "        accuracy))\n",
        "        torch.save(CNNmodel2_1.state_dict(), 'CNNmodel2_1.pt')\n",
        "        highest_accuracyModel2_1 = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLG8lBH2pnuk"
      },
      "outputs": [],
      "source": [
        "#learning rate 0.001 and loss function is Adam\n",
        "CNNmodel2_2 = FashionModel2() # init model\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate2 = 0.005\n",
        "optimizer4 = torch.optim.Adam(CNNmodel2_2.parameters(),lr=learning_rate2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3LJOSNK7E3e"
      },
      "outputs": [],
      "source": [
        "#Training model with params (lr = 0.001,optimizer = Adam)\n",
        "epochs = 50\n",
        "highest_accuracyModel2_2 = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel2_2(imgs) \n",
        "        loss = error(preds, lbls) \n",
        "        optimizer4.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer4.step() \n",
        "\n",
        "    for batch in test_loader:\n",
        "        imgs, lbls = batch\n",
        "        preds = CNNmodel2_2(imgs)\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_item(preds, lbls)\n",
        "        accuracy = (total_correct/len(test_set1)) * 100\n",
        "        \n",
        "    end_time = time.time() - start_time    \n",
        "    print(\"Epoch no.\",epoch+1 ,\"| accuracy: \", accuracy,\"%\", \"| total_correct: \", total_correct,\"/10000\" ,\"| epoch_dura: \", end_time,\"sec\")\n",
        "    \n",
        "    if accuracy >= highest_accuracyModel2_2:\n",
        "        print('Accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        highest_accuracyModel2_2,\n",
        "        accuracy))\n",
        "        torch.save(CNNmodel2_2.state_dict(), 'CNNmodel2_2.pt')\n",
        "        highest_accuracyModel2_2 = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs-hbfZsBKIh"
      },
      "outputs": [],
      "source": [
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPE1kbTpCT2V"
      },
      "outputs": [],
      "source": [
        "pred_data_loader = torch.utils.data.DataLoader(batch_size=1000, dataset=test_set1, num_workers=2)\n",
        "all_preds= get_all_preds(network=CNNmodel1_1, dataloader=pred_data_loader) \n",
        "plot_confusion_matrix(cm=confusion_matrix(y_true=test_set1.targets, y_pred=all_preds.argmax(1)), target_names=test_set1.classes, normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLC_lrvOAG8c"
      },
      "outputs": [],
      "source": [
        "pred_data_loader = torch.utils.data.DataLoader(batch_size=1000, dataset=test_set1, num_workers=2)\n",
        "all_preds= get_all_preds(network=CNNmodel1_2, dataloader=pred_data_loader) \n",
        "plot_confusion_matrix(cm=confusion_matrix(y_true=test_set1.targets, y_pred=all_preds.argmax(1)), target_names=test_set1.classes, normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceiTpeZB7li9"
      },
      "outputs": [],
      "source": [
        "pred_data_loader = torch.utils.data.DataLoader(batch_size=1000, dataset=test_set1, num_workers=1)\n",
        "all_preds= get_all_preds(network=CNNmodel2_1, dataloader=pred_data_loader) \n",
        "plot_confusion_matrix(cm=confusion_matrix(y_true=test_set1.targets, y_pred=all_preds.argmax(1)), target_names=test_set1.classes, normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFQ1IQqgrvCC"
      },
      "outputs": [],
      "source": [
        "pred_data_loader = torch.utils.data.DataLoader(batch_size=1000, dataset=test_set1, num_workers=1)\n",
        "all_preds= get_all_preds(network=CNNmodel2_2, dataloader=pred_data_loader) \n",
        "plot_confusion_matrix(cm=confusion_matrix(y_true=test_set1.targets, y_pred=all_preds.argmax(1)), target_names=test_set1.classes, normalize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh0a75d6_A1t"
      },
      "source": [
        "# Transfer Learning with our best CNN model, added SAM as optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3BWQ1MeRBb8"
      },
      "source": [
        "#Testing the model with an uploaded Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LcjvmGvmsVr"
      },
      "outputs": [],
      "source": [
        "model = CNNmodel1_1.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nocLaq_JL8e4"
      },
      "outputs": [],
      "source": [
        "import requests  \n",
        "from PIL import Image  \n",
        "import PIL.ImageOps  \n",
        "import cv2 as cv\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32Lil_Yfm6vT"
      },
      "outputs": [],
      "source": [
        "transform1=transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))]) \n",
        "def im_convert(tensor): \n",
        "  image=tensor.cpu().clone().detach().numpy() \n",
        "  image=image.transpose(1,2,0) \n",
        "  print(image.shape) \n",
        "  image=image*(np.array((0.5,0.5,0.5))+np.array((0.5,0.5,0.5))) \n",
        "  image=image.clip(0,1) \n",
        "  return image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rv6PmSyNpId"
      },
      "outputs": [],
      "source": [
        "url = \"https://media.istockphoto.com/id/483960103/photo/blank-black-t-shirt-front-with-clipping-path.jpg?s=1024x1024&w=is&k=20&c=n-53sp-rgdEyhzQLT5YTUEcafMj2qhf4Ke7mMWAQpxs=\" \n",
        "response=requests.get(url,stream=True) \n",
        "img=Image.open(response.raw) \n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYxjofNONsWM"
      },
      "outputs": [],
      "source": [
        "img=PIL.ImageOps.invert(img) \n",
        "img=img.convert('1') \n",
        "img=transform1(img) \n",
        "plt.imshow(im_convert(img)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHLlSi-jN_Qd"
      },
      "outputs": [],
      "source": [
        "images=img.to(device) \n",
        "images=images[0].unsqueeze(0).unsqueeze(0) \n",
        "output=model(images) \n",
        "_,pred=torch.max(output,1) \n",
        "print(\"The model has predicted : \",output_mapping[pred.item( )])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rF6Odv9Q1XV"
      },
      "outputs": [],
      "source": [
        "url = \"https://media.istockphoto.com/id/154417962/photo/red-luxury-leather-bag-on-white-background.jpg?s=1024x1024&w=is&k=20&c=muZaXn5nurAyyWUheNtY2_l_gk_OEVPYuuKAsoQESBg=\" \n",
        "response=requests.get(url,stream=True) \n",
        "img=Image.open(response.raw) \n",
        "plt.imshow(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz64OnuMRJn4"
      },
      "outputs": [],
      "source": [
        "img=PIL.ImageOps.invert(img) \n",
        "img=img.convert('1') \n",
        "img=transform1(img) \n",
        "plt.imshow(im_convert(img)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW0NHtW6VSet"
      },
      "outputs": [],
      "source": [
        "images=img.to(device) \n",
        "images=images[0].unsqueeze(0).unsqueeze(0) \n",
        "output=model(images) \n",
        "_,pred=torch.max(output,1) \n",
        "print(\"The model has predicted : \",output_mapping[pred.item( )])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12p-Y5JvVX2_"
      },
      "outputs": [],
      "source": [
        "url = \"https://media.istockphoto.com/id/934408078/photo/navy-blue-retro-dress-with-short-sleeves-isolated-on-white.jpg?s=1024x1024&w=is&k=20&c=0SBxR-KHbIik1rQfVtXE1JQKKRXZ2n5Km2al5TuFtjU=\" \n",
        "response=requests.get(url,stream=True) \n",
        "img=Image.open(response.raw) \n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXJO42f5Vbmr"
      },
      "outputs": [],
      "source": [
        "img=PIL.ImageOps.invert(img) \n",
        "img=img.convert('1') \n",
        "img=transform1(img) \n",
        "plt.imshow(im_convert(img)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGOONm8OVeVf"
      },
      "outputs": [],
      "source": [
        "images=img.to(device) \n",
        "images=images[0].unsqueeze(0).unsqueeze(0) \n",
        "output=model(images) \n",
        "_,pred=torch.max(output,1) \n",
        "print(\"The model has predicted : \",output_mapping[pred.item( )])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUA5miX_-_JG"
      },
      "outputs": [],
      "source": [
        "url = \"https://media.istockphoto.com/id/492462644/photo/green-dress-with-belt.jpg?s=1024x1024&w=is&k=20&c=XQLywLqbaJpHVr2XerbnvCSM5RDS-JFl4-ztmiQdDys=\" \n",
        "response=requests.get(url,stream=True) \n",
        "img=Image.open(response.raw) \n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Fgm_wY_LQy"
      },
      "outputs": [],
      "source": [
        "img=PIL.ImageOps.invert(img) \n",
        "img=img.convert('1') \n",
        "img=transform1(img) \n",
        "plt.imshow(im_convert(img)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VSOPd4t_N8K"
      },
      "outputs": [],
      "source": [
        "images=img.to(device) \n",
        "images=images[0].unsqueeze(0).unsqueeze(0) \n",
        "output=model(images) \n",
        "_,pred=torch.max(output,1) \n",
        "print(\"The model has predicted : \",output_mapping[pred.item( )])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSxvAVFE_QRL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "R19yERDu-AU7",
        "3nILFm-GckmL",
        "xK-7LSlOZxD7",
        "E549Z2sX3GCY",
        "jmsjZ_j16TpR",
        "snPzvsxP9jnb",
        "XQPjjIK_9t4a",
        "QSJl49xy3FvX",
        "ZHzfBqK_diLU",
        "abrP5UplU03c",
        "u1VWgCUKVs3B",
        "kb0iykizWmV3",
        "uCBw4AMDXM3y",
        "XZGcxSB_e5nP",
        "xHOFfPY4X4iV",
        "NRPYr_M0YKrs",
        "Bcv4Z5eUY-Ct",
        "ZIlgtZyoZNnG",
        "rLMTvWtcZudB",
        "She9LmZ-aFoo",
        "qzfl2yYvYlW-",
        "iW43L9IkbGwz",
        "IpD0Vg2WY_DI",
        "OBSCudn-bm73",
        "V5HTMtxYo3Yr",
        "7cWHWMeCcbVr",
        "9FfbkQE7cRMY",
        "xwiX-mmxE_zl"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}